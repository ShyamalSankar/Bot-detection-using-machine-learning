{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d16bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing of necessary packages\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "import demoji\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from numpy import loadtxt, savetxt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel, pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D, BatchNormalization, Bidirectional\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7325d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the dataset\n",
    "df_human = pd.read_csv('human_tweets_processed.csv')\n",
    "df_fake = pd.read_csv('bot_tweets_fake_processed.csv')\n",
    "df_social = pd.read_csv('bot_tweets_social_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e15b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_all = pd.concat([df_human, df_fake, df_social])\n",
    "df_tweets_all = df_tweets_all.reset_index(inplace = False, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "140a6921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>isBot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@KyleDavidHall @YouTube YASS  THANKS BABE</td>\n",
       "      <td>__user_mention__ __user_mention__ YASS THANKS ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @AbnInfVet: ...And Then Bloomberg Says He D...</td>\n",
       "      <td>RT __user_mention__ ...And Then Bloomberg Says...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @lgbtqnation: Federal judge considers separ...</td>\n",
       "      <td>RT __user_mention__ Federal judge considers se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @OKFosterWishes: URGENT; Beds needed to get...</td>\n",
       "      <td>RT __user_mention__ URGENT; Beds needed to get...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @luke_brooks: N.America and EU! Our EP \"Wou...</td>\n",
       "      <td>RT __user_mention__ N.America and EU! Our EP \"...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248177</th>\n",
       "      <td>theawkwardmoment#theawkwardmoment when you sta...</td>\n",
       "      <td>theawkwardmoment#theawkwardmoment when you sta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248178</th>\n",
       "      <td>I will have the rest of the site updated when ...</td>\n",
       "      <td>I will have the rest of the site updated when ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248179</th>\n",
       "      <td>RT @EMANSANGELS: Follow @scottstorch and view ...</td>\n",
       "      <td>RT __user_mention__ Follow __user_mention__ an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248180</th>\n",
       "      <td>Stupidity in numbers.  penn state riot</td>\n",
       "      <td>Stupidity in numbers. penn state riot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248181</th>\n",
       "      <td>RT @Srkrokx: Website Rank | Search Engine Opti...</td>\n",
       "      <td>RT __user_mention__ Website Rank | Search Engi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248182 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0               @KyleDavidHall @YouTube YASS  THANKS BABE   \n",
       "1       RT @AbnInfVet: ...And Then Bloomberg Says He D...   \n",
       "2       RT @lgbtqnation: Federal judge considers separ...   \n",
       "3       RT @OKFosterWishes: URGENT; Beds needed to get...   \n",
       "4       RT @luke_brooks: N.America and EU! Our EP \"Wou...   \n",
       "...                                                   ...   \n",
       "248177  theawkwardmoment#theawkwardmoment when you sta...   \n",
       "248178  I will have the rest of the site updated when ...   \n",
       "248179  RT @EMANSANGELS: Follow @scottstorch and view ...   \n",
       "248180             Stupidity in numbers.  penn state riot   \n",
       "248181  RT @Srkrokx: Website Rank | Search Engine Opti...   \n",
       "\n",
       "                                             cleaned_text  isBot  \n",
       "0       __user_mention__ __user_mention__ YASS THANKS ...      0  \n",
       "1       RT __user_mention__ ...And Then Bloomberg Says...      0  \n",
       "2       RT __user_mention__ Federal judge considers se...      0  \n",
       "3       RT __user_mention__ URGENT; Beds needed to get...      0  \n",
       "4       RT __user_mention__ N.America and EU! Our EP \"...      0  \n",
       "...                                                   ...    ...  \n",
       "248177  theawkwardmoment#theawkwardmoment when you sta...      1  \n",
       "248178  I will have the rest of the site updated when ...      1  \n",
       "248179  RT __user_mention__ Follow __user_mention__ an...      1  \n",
       "248180              Stupidity in numbers. penn state riot      1  \n",
       "248181  RT __user_mention__ Website Rank | Search Engi...      1  \n",
       "\n",
       "[248182 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets = df_tweets_all[[\"text\", \"cleaned_text\", \"bot\"]]\n",
    "df_tweets = df_tweets.rename(columns = {\"bot\": \"isBot\"})\n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7c3a04",
   "metadata": {},
   "source": [
    "# Model\n",
    "1. Logistic Regression (BoW)\n",
    "2. Random Forest Classifier (BoW)\n",
    "3. Logistic Regression (TF-IDF)\n",
    "4. Random Forest Classifier (TF-IDF)\n",
    "5. LSTM\n",
    "6. Logistic Regression (BERT)\n",
    "7. Random Forest Classifier (BERT)\n",
    "8. Adaboost Classifier (BERT)\n",
    "9. XGBoost (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065976e0",
   "metadata": {},
   "source": [
    "## BERT Models\n",
    "While previously the word embeddings from Glove are not fully context dependent, let us consider an embedding that considers both context and both directions, ie the BERT word embedding\n",
    "\n",
    "Here, first, we generate the BERT feature vector for each tweet and then we train models on those feature representations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29709ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#initialising a pretrained bert model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", padding = True)\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "nlp = pipeline(\"feature-extraction\", tokenizer = tokenizer, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee7883f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#store all the tweets after they have been encoded by BERT\n",
    "\n",
    "# feature_list = []\n",
    "\n",
    "# for index, row in df_tweets_searches.iterrows():\n",
    "#     #extracting the ith tweet and restricting the characters to 512, which is fine because twitter's limit is 280\n",
    "#     text = row['cleaned_text'][:512]\n",
    "#     #encoding all the individual words present in the tweet\n",
    "#     vec = np.array(nlp(text))\n",
    "#     #getting the mean representation of the words present in the tweet\n",
    "#     vec = vec.reshape((vec.shape[1], vec.shape[2])).mean(axis = 0)\n",
    "#     feature_list.append(vec)\n",
    "\n",
    "# feature_vectors = np.array(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62a604f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = pd.read_csv(\"feature_vect.csv\")\n",
    "feature_vectors = feature_vectors.drop(columns = \"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d0b0d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the generated features into training and testing features\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_vectors, df_tweets['isBot'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d5da4",
   "metadata": {},
   "source": [
    "## 06 Logistic Regression (BERT)\n",
    "* Model\n",
    "* Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccbd6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "log_regression = LogisticRegression(max_iter = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0016d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Fit Model\n",
    "log_model = log_regression.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0304a59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# y_prediction\n",
    "y_pred = log_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5cb592d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7675524306464936\n",
      "Log Loss: 8.028520023572051\n",
      "ROC AUC: 0.7160054008279689\n",
      "F1-score: 0.6185785123966943\n",
      "Precision: 0.697376267143709\n",
      "Recall: 0.5557799691101343\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.83     32803\n",
      "           1       0.70      0.56      0.62     16834\n",
      "\n",
      "    accuracy                           0.77     49637\n",
      "   macro avg       0.75      0.72      0.73     49637\n",
      "weighted avg       0.76      0.77      0.76     49637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Error Metrics\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "lg_loss = metrics.log_loss(y_test, y_pred)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "print(f'Recall: {recall}')\n",
    "report = metrics.classification_report(y_test, y_pred)\n",
    "print(f'Classification Report: \\n {report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f487a",
   "metadata": {},
   "source": [
    "## 07 Random Forest Classifier (BERT)\n",
    "* Model\n",
    "* Error Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413654fe",
   "metadata": {},
   "source": [
    "### Model (Optimal Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1354b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model (initialise the object based on parameters selected by random search)\n",
    "rf_classifier = RandomForestClassifier(bootstrap = False, \n",
    "                                       max_depth = 80, \n",
    "                                       max_features = \"auto\", \n",
    "                                       min_samples_split = 10, \n",
    "                                       n_estimators = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62b9af20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 13min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Fit Model\n",
    "rf_model = rf_classifier.fit(x_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "052f492b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# y_prediction for the best model\n",
    "y_pred_optimal_rf = rf_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd4dccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9551342748353043\n",
      "Log Loss: 1.5496120336020083\n",
      "ROC AUC: 0.9381485126352425\n",
      "F1-score: 0.9304822850007805\n",
      "Precision: 0.9804618117229129\n",
      "Recall: 0.8853510752049424\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97     32803\n",
      "           1       0.98      0.89      0.93     16834\n",
      "\n",
      "    accuracy                           0.96     49637\n",
      "   macro avg       0.96      0.94      0.95     49637\n",
      "weighted avg       0.96      0.96      0.95     49637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Metrics for tuned random forest\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_optimal_rf)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "lg_loss = metrics.log_loss(y_test, y_pred_optimal_rf)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_optimal_rf)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_optimal_rf)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred_optimal_rf)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_optimal_rf)\n",
    "print(f'Recall: {recall}')\n",
    "report = metrics.classification_report(y_test, y_pred_optimal_rf)\n",
    "print(f'Classification Report: \\n {report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b2b15",
   "metadata": {},
   "source": [
    "## 08 Adaboost Classifier (BERT)\n",
    "* Model\n",
    "* Error Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681997df",
   "metadata": {},
   "source": [
    "### Model (Optimal Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04c2dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model (initialise the object based on parameters selected by random search)\n",
    "adaboost_classifier = AdaBoostClassifier(n_estimators = 200, \n",
    "                                         learning_rate = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebef495a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 50min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Fit Model\n",
    "adaboost_model = adaboost_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba3ed5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# y_prediction for the best model\n",
    "y_pred_optimal_ada = adaboost_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f761e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7529665370590487\n",
      "Log Loss: 8.53230676963379\n",
      "ROC AUC: 0.701528514821457\n",
      "F1-score: 0.5979408485802348\n",
      "Precision: 0.6673009367681498\n",
      "Recall: 0.5416419151716764\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82     32803\n",
      "           1       0.67      0.54      0.60     16834\n",
      "\n",
      "    accuracy                           0.75     49637\n",
      "   macro avg       0.73      0.70      0.71     49637\n",
      "weighted avg       0.75      0.75      0.75     49637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_optimal_ada)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "lg_loss = metrics.log_loss(y_test, y_pred_optimal_ada)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_optimal_ada)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_optimal_ada)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred_optimal_ada)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_optimal_ada)\n",
    "print(f'Recall: {recall}')\n",
    "report = metrics.classification_report(y_test, y_pred_optimal_ada)\n",
    "print(f'Classification Report: \\n {report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0234f5a9",
   "metadata": {},
   "source": [
    "## 09 XGBoost Classifier (BERT)\n",
    "* Model\n",
    "* Error Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464397c6",
   "metadata": {},
   "source": [
    "### Model (Optimal Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1ce1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model (initialise the object based on parameters selected by bayesian optimisation)\n",
    "weight_train = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "\n",
    "xgb_classifier = XGBClassifier(gamma = 0.1,\n",
    "                               alpha = 0.5,\n",
    "                               max_depth = 15, \n",
    "                               eta = 0.5, \n",
    "                               subsample = 0.8,\n",
    "                               colsample_bytree = 0.7,\n",
    "                               scale_pos_weight = weight_train,\n",
    "                               objective = \"binary:logistic\",\n",
    "                               eval_metric = \"logloss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc00a4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 40min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Fit Model\n",
    "xgb_model = xgb_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6db60a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# y_prediction for the best model\n",
    "y_pred_optimal_xgb = xgb_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df7edc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9318250498619981\n",
      "Log Loss: 2.3547112541710735\n",
      "ROC AUC: 0.9281185147921562\n",
      "F1-score: 0.9011797687186077\n",
      "Precision: 0.8862722573233773\n",
      "Recall: 0.9165973624806938\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95     32803\n",
      "           1       0.89      0.92      0.90     16834\n",
      "\n",
      "    accuracy                           0.93     49637\n",
      "   macro avg       0.92      0.93      0.92     49637\n",
      "weighted avg       0.93      0.93      0.93     49637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_optimal_xgb)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "lg_loss = metrics.log_loss(y_test, y_pred_optimal_xgb)\n",
    "print(f'Log Loss: {lg_loss}')\n",
    "roc_auc = metrics.roc_auc_score(y_test, y_pred_optimal_xgb)\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "f1_score = metrics.f1_score(y_test, y_pred_optimal_xgb)\n",
    "print(f'F1-score: {f1_score}')\n",
    "precision = metrics.precision_score(y_test, y_pred_optimal_xgb)\n",
    "print(f'Precision: {precision}')\n",
    "recall = metrics.recall_score(y_test, y_pred_optimal_xgb)\n",
    "print(f'Recall: {recall}')\n",
    "report = metrics.classification_report(y_test, y_pred_optimal_xgb)\n",
    "print(f'Classification Report: \\n {report}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
