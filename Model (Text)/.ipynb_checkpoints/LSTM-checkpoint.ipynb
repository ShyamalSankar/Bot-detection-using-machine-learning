{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13186460",
   "metadata": {},
   "source": [
    "# In this notebook, we shall train LSTM models for tweet level classification\n",
    "\n",
    "We shall aim to train 2 types of models, one LSTM that purely makes use of textual data and another LSTM that uses the metadata that comes with the tweet level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c4d48600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, recall_score, precision_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D, BatchNormalization, Bidirectional, Input, Concatenate, concatenate, Masking\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, pipeline\n",
    "import gensim\n",
    "from gensim import downloader\n",
    "import pickle\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8daa7be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the data\n",
    "tweets = pd.read_csv('tweets_dataset_allcols.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c843bd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>user_id</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_urls</th>\n",
       "      <th>num_mentions</th>\n",
       "      <th>created_at</th>\n",
       "      <th>bot</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>isBot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>558857025270988801</td>\n",
       "      <td>aleah is me http://t.co/PXwTMPzLFf</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>2.286244e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sat Jan 24 05:21:12 +0000 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>aleah is me</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>361688175418150913</td>\n",
       "      <td>@WittyOfficial I got you bruh</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>2.575995e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Mon Jul 29 03:22:35 +0000 2013</td>\n",
       "      <td>0</td>\n",
       "      <td>__user_mention__ I got you bruh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109649117361344513</td>\n",
       "      <td>@MissKotzyba its a diet where u can only eat p...</td>\n",
       "      <td>&lt;a href=\"http://blackberry.com/twitter\" rel=\"n...</td>\n",
       "      <td>1.093557e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Fri Sep 02 15:29:21 +0000 2011</td>\n",
       "      <td>1</td>\n",
       "      <td>__user_mention__ its a diet where u can only e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>252533532839260161</td>\n",
       "      <td>When we are no longer able to change a situati...</td>\n",
       "      <td>web</td>\n",
       "      <td>6.176962e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sun Sep 30 22:21:00 +0000 2012</td>\n",
       "      <td>1</td>\n",
       "      <td>When we are no longer able to change a situati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>576292622286749696</td>\n",
       "      <td>@cat_lover_996 @Lauri777Ellonen @Wendy_Rich_UK...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>2.223970e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Fri Mar 13 08:04:02 +0000 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>__user_mention__ __user_mention__ __user_menti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0  558857025270988801                 aleah is me http://t.co/PXwTMPzLFf   \n",
       "1  361688175418150913                      @WittyOfficial I got you bruh   \n",
       "2  109649117361344513  @MissKotzyba its a diet where u can only eat p...   \n",
       "3  252533532839260161  When we are no longer able to change a situati...   \n",
       "4  576292622286749696  @cat_lover_996 @Lauri777Ellonen @Wendy_Rich_UK...   \n",
       "\n",
       "                                              source       user_id  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...  2.286244e+09   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...  2.575995e+07   \n",
       "2  <a href=\"http://blackberry.com/twitter\" rel=\"n...  1.093557e+07   \n",
       "3                                                web  6.176962e+08   \n",
       "4  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...  2.223970e+09   \n",
       "\n",
       "   retweeted_status_id  retweet_count  reply_count  favorite_count  \\\n",
       "0                  0.0            0.0          0.0             0.0   \n",
       "1                  0.0            0.0          0.0             0.0   \n",
       "2                  NaN            0.0          0.0             0.0   \n",
       "3                  NaN            0.0          0.0             0.0   \n",
       "4                  0.0            0.0          0.0             0.0   \n",
       "\n",
       "   num_hashtags  num_urls  num_mentions                      created_at  bot  \\\n",
       "0           0.0       0.0           0.0  Sat Jan 24 05:21:12 +0000 2015    0   \n",
       "1           0.0       0.0           1.0  Mon Jul 29 03:22:35 +0000 2013    0   \n",
       "2           0.0       1.0           1.0  Fri Sep 02 15:29:21 +0000 2011    1   \n",
       "3           0.0       0.0           0.0  Sun Sep 30 22:21:00 +0000 2012    1   \n",
       "4           0.0       0.0           4.0  Fri Mar 13 08:04:02 +0000 2015    0   \n",
       "\n",
       "                                        cleaned_text  isBot  \n",
       "0                                       aleah is me       0  \n",
       "1                    __user_mention__ I got you bruh      0  \n",
       "2  __user_mention__ its a diet where u can only e...      1  \n",
       "3  When we are no longer able to change a situati...      1  \n",
       "4  __user_mention__ __user_mention__ __user_menti...      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51044205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25000\n",
       "1    25000\n",
       "Name: bot, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As we can see, the tweets are already \n",
    "tweets.bot.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48d2a4",
   "metadata": {},
   "source": [
    "# Metadata only model\n",
    "In this section, we shall use only the meta data associated with each individual tweet (as opposed to account level data, which is handled in our other section) in order to see if patterns in the metadata is sufficient to distinguish bot generated tweets from human generated tweets. The features that we are going to use is retweet_count\treply_count\tfavorite_count\tnum_hashtags\tnum_urls\tnum_mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ead32698",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets[[\"retweet_count\",\"reply_count\", \"favorite_count\", \"num_hashtags\", \"num_urls\", \"num_mentions\"]]\n",
    "y = tweets[['bot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e4c18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "719f0666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The classes are already balanced, so we\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "log_reg.fit(X_train, y_train.bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cb58c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.62      0.67      5000\n",
      "           1       0.67      0.77      0.72      5000\n",
      "\n",
      "    accuracy                           0.69     10000\n",
      "   macro avg       0.70      0.69      0.69     10000\n",
      "weighted avg       0.70      0.69      0.69     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c172f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.60      0.66     20000\n",
      "           1       0.66      0.77      0.71     20000\n",
      "\n",
      "    accuracy                           0.68     40000\n",
      "   macro avg       0.69      0.68      0.68     40000\n",
      "weighted avg       0.69      0.68      0.68     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking for overfitting\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "#As can be seen below, training set and test set performance are similar, indicating that there is no overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fcad26",
   "metadata": {},
   "source": [
    "As we can see from the classification report above, the logistic regression model is able to perform much better than random acheiving precision, recall and f1 scores of around 0.7. \n",
    "\n",
    "This shows that tweet level meta features do have predictive power in determining whether a tweet is generated by a bot or a human."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd2f31",
   "metadata": {},
   "source": [
    "# Vanilla LSTM\n",
    "In this section, the LSTM model that we create simply uses a standard LSTM architecture and only takes into account the textual data in a tweet to determine whether it is generated by a human or a bot. First, we try using a self trained w2v, then a pretrained glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6138ae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, we shall use a self trained word 2 vec and train an LSTM model\n",
    "X = tweets[['cleaned_text']]\n",
    "y = tweets[['bot']]\n",
    "\n",
    "#Before training word to vec, we shall split into train test split to avoid data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3ef676de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sentence = [d.split() for d in X_train['cleaned_text'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1f0c12e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we shall try training a 50d w2v model first\n",
    "#create the word to vector\n",
    "#try a 50 dimensional vector first\n",
    "dim = 50\n",
    "#Fit a Word2Vec model on our dataset\n",
    "w2v = gensim.models.Word2Vec(sentences = X_train_sentence, vector_size = dim, window = 10, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e5106a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "52cfaef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the words (this is for padding later), basically converts the sentences into lists of numbers in which\n",
    "#will be used in the embedding layer of the LSTM\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "235a1b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(x) for x in X_train_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9032a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we can pad the sequences, because the Neural net takes in constant length vectors\n",
    "X_train_tokens = pad_sequences(X_train_tokens, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0e659eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for determining the embedding matrix to be used in the LSTM\n",
    "vocab = tokenizer.word_index\n",
    "vocabulary_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ede4426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the embedding matrix according to our word embeddings, this matrix is to store all the word embeddings according to\n",
    "#our training above (each word gets its own vector)\n",
    "embedding_mat = np.zeros((vocabulary_size, dim))\n",
    "num_words = 0\n",
    "#insert the word embeddings into our initialised object\n",
    "for word, token in tokenizer.word_index.items():\n",
    "\n",
    "    #get the corresponding vector to the word and the token\n",
    "    #if the word is present in the dictionary, then we append the corresponding matrix else we continue\n",
    "    if word in w2v.wv:\n",
    "        #access the vector embedding that was generated by our word2vec model\n",
    "        vector = w2v.wv[word]\n",
    "        embedding_mat[token] = vector\n",
    "        num_words += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0970b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the LSTM model\n",
    "\n",
    "inputs = Input(shape = (max_length,))\n",
    "#we do not want our embedding layer to be traininable\n",
    "embedding = Embedding(input_dim = vocabulary_size, output_dim = dim, input_length = max_length, weights = [embedding_mat],\n",
    "                     trainable = False)(inputs)\n",
    "#next, we add the lstm layer \n",
    "lstm = LSTM(units = 32, recurrent_dropout = 0.3)(embedding)\n",
    "#finally, add 2 dense layers\n",
    "dense1 = Dense(units = 128, activation = 'relu')(lstm)\n",
    "dense2 = Dense(units = 64, activation = 'relu') (dense1)\n",
    "#The final cell is the output cell with a softmax \n",
    "output = Dense(units = 1, activation = 'sigmoid')(dense2)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = output)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9bf51213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1250/1250 [==============================] - 19s 13ms/step - loss: 0.6000 - acc: 0.6795\n",
      "Epoch 2/8\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5698 - acc: 0.7074\n",
      "Epoch 3/8\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5593 - acc: 0.7139\n",
      "Epoch 4/8\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5537 - acc: 0.7161\n",
      "Epoch 5/8\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5484 - acc: 0.7202\n",
      "Epoch 6/8\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 0.5423 - acc: 0.7231\n",
      "Epoch 7/8\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 0.5385 - acc: 0.7254\n",
      "Epoch 8/8\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 0.5352 - acc: 0.7271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x279ad4b2d90>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_tokens, y_train, epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7d1b188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting tokenizations for test data \n",
    "X_test_sentence = [d.split() for d in X_test['cleaned_text'].tolist()]\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "63770929",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokens = pad_sequences(X_test_tokens, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8a562fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d501c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.where(y_pred_proba > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f07c1278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.68      0.71      5000\n",
      "           1       0.71      0.78      0.74      5000\n",
      "\n",
      "    accuracy                           0.73     10000\n",
      "   macro avg       0.73      0.73      0.73     10000\n",
      "weighted avg       0.73      0.73      0.73     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa2b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are able to obtain 0.73 accuracy by using our own pretrained word2vec model. However, the data we have to train this vector \n",
    "#representation is rather small at only 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa22975",
   "metadata": {},
   "source": [
    "# LSTM with pretrained glove.\n",
    "Here, we use the pretrained glove by stanford glove (https://nlp.stanford.edu/projects/glove/). Here, the authors have trained their vector representations on billions of tweets and it is likely that they are able to obtain superior text representations as compared to our self trained word to vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3467cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, the glove cannot handle punctuated words and does not contain punctuation, so we shall find and replace all such occurences\n",
    "short_form_dict = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",}\n",
    "\n",
    "def replace_short_form(text):\n",
    "    for key, value in short_form_dict.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf29035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_100 = downloader.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ca6f0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing steps for glove (taken from and adjusted https://www.kaggle.com/code/amackcrane/python-version-of-glove-twitter-preprocess-script)\n",
    "import sys\n",
    "import regex as re\n",
    "\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = \"<hashtag> {} <allcaps>\".format(hashtag_body.lower())\n",
    "    else:\n",
    "        result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "    return result\n",
    "\n",
    "def allcaps(text):\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps> \" # amackcrane added trailing space\n",
    "\n",
    "\n",
    "def preprocess_glove(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"#\\w+\", hashtag)  # amackcrane edit\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    text = re_sub(r\"([a-zA-Z<>()])([?!.:;,])\", r\"\\1 \\2\")\n",
    "    text = re_sub(r\"\\(([a-zA-Z<>]+)\\)\", r\"( \\1 )\")\n",
    "    text = re_sub(r\"  \", r\" \")\n",
    "    text = re_sub(r\" ([A-Z]){2,} \", allcaps)\n",
    "    text = replace_short_form(text)\n",
    "    punct =['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '.',\n",
    "           '/', ':', ';', '=', '?', '@', '[', '\\\\', ']', '^', '_', \n",
    "           '`', '{', '|', '}', '~', '»', '«', '“', '”', '—', ]\n",
    "    punct_pattern = re.compile(\"[\" + re.escape(\"\".join(punct)) + \"]\")\n",
    "    text = re.sub(punct_pattern, \"\", text)\n",
    "    text = text.replace(\"…\", \"\")\n",
    "    text = text.replace(\"<number>th\", \"<number>\")\n",
    "    text = text.replace(\"><\", \"> <\")\n",
    "    pattern = re.compile(r'A1.8301$')\n",
    "    text = text.replace(\"♥\", \"<heart>\")\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "13a121b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['glove_preprocessed'] = tweets.text.apply(preprocess_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "614210e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, we shall use a self trained word 2 vec and train an LSTM model (include the metadata features here for the next section\n",
    "#where the LSTM makes use of both metadata features and text features)\n",
    "X = tweets[[\"retweet_count\",\"reply_count\", \"favorite_count\", \"num_hashtags\", \"num_urls\", \"num_mentions\",'glove_preprocessed']]\n",
    "y = tweets[['bot']]\n",
    "\n",
    "#Before training word to vec, we shall split into train test split to avoid data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5b377a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sentence = [d.split() for d in X_train['glove_preprocessed'].tolist()]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4556fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the words (this is for padding later), basically converts the sentences into lists of numbers in which\n",
    "#will be used in the embedding layer of the LSTM\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9cd3e17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(x) for x in X_train_tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "47048a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a8dff304",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = pad_sequences(X_train_tokens, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ff6b7abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 100\n",
    "embedding_mat_glove = np.zeros((vocabulary_size, dim))\n",
    "num_words = 0\n",
    "not_in_vocab = 0\n",
    "#insert the word embeddings into our initialised object\n",
    "for word, token in tokenizer.word_index.items():\n",
    "    \n",
    "    #first, we remove any punctuation from the words and store a modified version of the word\n",
    "    #word_modified = ''.join(c for c in word if c.isalnum() or c == '<' or c == '>')\n",
    "    #get the corresponding vector to the word and the token\n",
    "    #if the word is present in the dictionary, then we append the corresponding matrix else we continue\n",
    "    if word in glove_100:\n",
    "        vector = glove_100[word]\n",
    "        embedding_mat_glove[token] = vector\n",
    "        num_words +=1\n",
    "    else:\n",
    "        not_in_vocab += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ee8d83eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the LSTM model\n",
    "\n",
    "inputs = Input(shape = (max_length,))\n",
    "#we do not want our embedding layer to be traininable\n",
    "embedding = Embedding(input_dim = vocabulary_size, output_dim = dim, input_length = max_length, weights = [embedding_mat_glove],\n",
    "                     trainable = False)(inputs)\n",
    "#next, we add the lstm layer \n",
    "lstm = LSTM(units = 32, recurrent_dropout = 0.3)(embedding)\n",
    "#finally, add 2 dense layers\n",
    "dense1 = Dense(units = 128, activation = 'relu')(lstm)\n",
    "dense2 = Dense(units = 64, activation = 'relu') (dense1)\n",
    "#The final cell is the output cell with a softmax \n",
    "output = Dense(units = 1, activation = 'sigmoid')(dense2)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = output)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1b15cca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1250/1250 [==============================] - 21s 14ms/step - loss: 0.5547 - acc: 0.7123\n",
      "Epoch 2/8\n",
      "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4946 - acc: 0.7594\n",
      "Epoch 3/8\n",
      "1250/1250 [==============================] - 18s 15ms/step - loss: 0.4656 - acc: 0.7786\n",
      "Epoch 4/8\n",
      "1250/1250 [==============================] - 18s 14ms/step - loss: 0.4445 - acc: 0.7930\n",
      "Epoch 5/8\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.4255 - acc: 0.8023\n",
      "Epoch 6/8\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.4065 - acc: 0.8149\n",
      "Epoch 7/8\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.3905 - acc: 0.8217\n",
      "Epoch 8/8\n",
      "1250/1250 [==============================] - 19s 16ms/step - loss: 0.3746 - acc: 0.8290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x279d51fbdc0>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_tokens, y_train.bot, epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "75a58530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Xtest tokens to see performance on test set\n",
    "\n",
    "X_test_sentence = [d.split() for d in X_test['glove_preprocessed'].tolist()]\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "44e3d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokens = pad_sequences(X_test_tokens, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "de9e0cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ea344843",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.where(y_pred_proba > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "11b78af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.72      0.75      5000\n",
      "           1       0.74      0.81      0.77      5000\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.77      0.76      0.76     10000\n",
      "weighted avg       0.77      0.76      0.76     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d696fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdb39c5",
   "metadata": {},
   "source": [
    "# Metadata + text LSTM\n",
    "From previous sections, we can see that both the textual content of the tweets and the metadata of the tweets have predictive power in classifying whether a tweet is generated from a bot or a human. Now, we shall construct an LSTM that makes use of both the textual content of a tweet and the metadata of a tweet and attempt to classify whether it was generated by a bot or a human.\n",
    "\n",
    "For this part, we shall continue to use the pretrained glove text representations since performance was superior as compared to self trained word 2 vec.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "766e1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting and scaling the metadata features\n",
    "\n",
    "sc = StandardScaler()\n",
    "metadata = X_train[[\"retweet_count\",\"reply_count\", \"favorite_count\", \"num_hashtags\", \"num_urls\", \"num_mentions\"]]\n",
    "\n",
    "#Fit the standard scaler on training data only to avoid data leakage\n",
    "metadata_scaled = sc.fit_transform(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16f20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a02337e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the nn architecture:\n",
    "\n",
    "#here, we have multiple inputs (max len because each input is a list of tokens)\n",
    "input_tweet = Input(shape = (max_length,))\n",
    "#The list of tokens is passed to an embedding layer, where the pretrained glove embeddings for each word are stored accessed\n",
    "#by the respective tokens before being passed into the LSTM layer\n",
    "embedding = Embedding(input_dim = vocabulary_size, output_dim = dim, input_length = max_length, weights = [embedding_mat_glove],\n",
    "                     trainable = False)(input_tweet)\n",
    "lstm = LSTM(units = 32)(embedding)\n",
    "\n",
    "#for the 6 features that we have\n",
    "input_metadata = Input(shape = (6,))\n",
    "combined = concatenate([input_metadata, lstm])\n",
    "#Then pass the combined input to a dense layer\n",
    "dense1 = Dense(128, activation = 'relu')(combined)\n",
    "dense2 = Dense(64, activation = 'relu')(dense1)\n",
    "output = Dense(1, activation = 'sigmoid')(dense2)\n",
    "\n",
    "model = Model(inputs = [input_tweet, input_metadata], outputs = [output])\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "20fe05d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "1250/1250 [==============================] - 15s 11ms/step - loss: 0.4294 - acc: 0.8040\n",
      "Epoch 2/12\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.3932 - acc: 0.8214\n",
      "Epoch 3/12\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.3677 - acc: 0.8357\n",
      "Epoch 4/12\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.3464 - acc: 0.8467\n",
      "Epoch 5/12\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.3265 - acc: 0.8564\n",
      "Epoch 6/12\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.3067 - acc: 0.8667\n",
      "Epoch 7/12\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.2914 - acc: 0.8735\n",
      "Epoch 8/12\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.2770 - acc: 0.8817\n",
      "Epoch 9/12\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2611 - acc: 0.8895\n",
      "Epoch 10/12\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 0.2462 - acc: 0.8963\n",
      "Epoch 11/12\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.2343 - acc: 0.9007\n",
      "Epoch 12/12\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.2221 - acc: 0.9086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x279b21a9a90>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = [X_train_tokens, metadata_scaled], y = y_train.bot, epochs = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "bb7452f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the test set metadata\n",
    "metadata_test = X_test[[\"retweet_count\",\"reply_count\", \"favorite_count\", \"num_hashtags\", \"num_urls\", \"num_mentions\"]]\n",
    "\n",
    "meta_test_scaled = sc.transform(metadata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7eafcc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict([X_test_tokens, meta_test_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a082e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.where(y_pred_proba > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "121cad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79      5000\n",
      "           1       0.80      0.76      0.78      5000\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.78      0.78      0.78     10000\n",
      "weighted avg       0.78      0.78      0.78     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50266d2",
   "metadata": {},
   "source": [
    "### As we can see from the classification report above, the combined LSTM performs slightly better than the pure textual data model and significantly better than the logistic regression model that uses only metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64c5ed",
   "metadata": {},
   "source": [
    "# Bert embeddings\n",
    "In this section, we shall try fitting LSTM models on bert embeddings. Previously we used Glove and Word2Vec which are non contextual word embeddings. In other words, we just get one numeric representation of all same words, ie the word bank always has one vector representation despite the possibility that the word bank is used in different contexts. However, BERT can capture different embeddings for each word depending on the context that it is used. Hence, we shall now train an LSTM using BERT embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f96c7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, we shall use a self trained word 2 vec and train an LSTM model (include the metadata features here for the next section\n",
    "#where the LSTM makes use of both metadata features and text features)\n",
    "X = tweets[[\"retweet_count\",\"reply_count\", \"favorite_count\", \"num_hashtags\", \"num_urls\", \"num_mentions\",'cleaned_text']]\n",
    "y = tweets[['bot']]\n",
    "\n",
    "#Before training word to vec, we shall split into train test split to avoid data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f6b82fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6816502627a44fd4b564fe13b60734c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veena\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\veena\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45e5f3b80a24f5fb3cffcd8ac3648ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/285 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6723b03f556f43b09db10e6948007909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#Here, we shall initialize a pretrained bert model (smaller dimensions than the other one)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"prajjwal1/bert-tiny\", padding = True)\n",
    "model = BertModel.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "nlp = pipeline(\"feature-extraction\", tokenizer = tokenizer, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad60cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the bert embeddings\n",
    "\n",
    "#store all the tweets after they have been encoded by BERT\n",
    "\n",
    "feature_list = []\n",
    "\n",
    "max_dim = 0\n",
    "for index, row in X_train.iterrows():\n",
    "    #extracting the ith tweet and restricting the characters to 512, which is fine because twitter's limit is 280\n",
    "    text = row['cleaned_text'][:512]\n",
    "    #encoding all the individual words present in the tweet\n",
    "    vec = np.array(nlp(text))\n",
    "\n",
    "    shp = vec.shape[1]\n",
    "    vec = np.pad(vec, ((0,0),(0,120 - shp),(0,0)), mode = 'constant', constant_values = -10)\n",
    "#   #getting the mean representation of the words present in the tweet\n",
    "\n",
    "    feature_list.append(vec)\n",
    "    \n",
    "\n",
    "feature_vectors_train = np.array(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91d83e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_test = []\n",
    "\n",
    "max_dim = 0\n",
    "for index, row in X_test.iterrows():\n",
    "    #extracting the ith tweet and restricting the characters to 512, which is fine because twitter's limit is 280\n",
    "    text = row['cleaned_text'][:512]\n",
    "    #encoding all the individual words present in the tweet\n",
    "    vec = np.array(nlp(text))\n",
    "\n",
    "    shp = vec.shape[1]\n",
    "    vec = np.pad(vec, ((0,0),(0,120 - shp),(0,0)), mode = 'constant', constant_values = -10)\n",
    "#   #getting the mean representation of the words present in the tweet\n",
    "\n",
    "    feature_list_test.append(vec)\n",
    "    \n",
    "\n",
    "feature_vectors_test = np.array(feature_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f35c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors_test = feature_vectors_test.reshape((10000, 120, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "faa2027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors_train = feature_vectors_train.reshape(40000, 120, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4489b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 120, 128)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vectors_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "686a20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model for bert embedding\n",
    "\n",
    "input_tweet = Input(shape = (120, 128))\n",
    "mask = Masking(mask_value=-10)(input_tweet)\n",
    "lstm = LSTM(32)(mask)\n",
    "dense1 = Dense(units = 128, activation = 'relu')(lstm)\n",
    "dense2 = Dense(units = 64, activation = 'relu') (dense1)\n",
    "#The final cell is the output cell with a softmax \n",
    "output = Dense(units = 1, activation = 'sigmoid')(dense2)\n",
    "\n",
    "model = Model(inputs = input_tweet, outputs = output)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a624f0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1250/1250 [==============================] - 83s 60ms/step - loss: 0.5836 - acc: 0.6873\n",
      "Epoch 2/15\n",
      "1250/1250 [==============================] - 70s 56ms/step - loss: 0.5254 - acc: 0.7327\n",
      "Epoch 3/15\n",
      "1250/1250 [==============================] - 70s 56ms/step - loss: 0.5042 - acc: 0.7478\n",
      "Epoch 4/15\n",
      "1250/1250 [==============================] - 71s 56ms/step - loss: 0.4866 - acc: 0.7598\n",
      "Epoch 5/15\n",
      "1250/1250 [==============================] - 62s 50ms/step - loss: 0.4720 - acc: 0.7710\n",
      "Epoch 6/15\n",
      "1250/1250 [==============================] - 66s 53ms/step - loss: 0.4579 - acc: 0.7801\n",
      "Epoch 7/15\n",
      "1250/1250 [==============================] - 63s 51ms/step - loss: 0.4447 - acc: 0.7890\n",
      "Epoch 8/15\n",
      "1250/1250 [==============================] - 68s 55ms/step - loss: 0.4302 - acc: 0.7993\n",
      "Epoch 9/15\n",
      "1250/1250 [==============================] - 63s 50ms/step - loss: 0.4185 - acc: 0.8070\n",
      "Epoch 10/15\n",
      "1250/1250 [==============================] - 67s 53ms/step - loss: 0.4069 - acc: 0.8138\n",
      "Epoch 11/15\n",
      "1250/1250 [==============================] - 72s 58ms/step - loss: 0.3952 - acc: 0.8198\n",
      "Epoch 12/15\n",
      "1250/1250 [==============================] - 68s 54ms/step - loss: 0.3830 - acc: 0.8275\n",
      "Epoch 13/15\n",
      "1250/1250 [==============================] - 77s 61ms/step - loss: 0.3673 - acc: 0.8354\n",
      "Epoch 14/15\n",
      "1250/1250 [==============================] - 88s 71ms/step - loss: 0.3574 - acc: 0.8430\n",
      "Epoch 15/15\n",
      "1250/1250 [==============================] - 87s 69ms/step - loss: 0.3456 - acc: 0.8463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19f848b0c40>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = feature_vectors_train, y = y_train.bot, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93aba0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 10s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict(feature_vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3563eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.where(y_pred_prob > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1add453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.76      5000\n",
      "           1       0.77      0.73      0.75      5000\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.76      0.76      0.76     10000\n",
      "weighted avg       0.76      0.76      0.76     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a5e43c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7570000000000001\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3c8d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now training a combined model\n",
    "meta_data_train = X_train.drop('cleaned_text', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d033824",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67cac12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_train[['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions']] = sc.fit_transform(meta_data_train[['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fcd5713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_test = X_test.drop('cleaned_text', axis = 1)\n",
    "meta_data_test[['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions']] = sc.transform(meta_data_test[['retweet_count', 'reply_count', 'favorite_count', 'num_hashtags', 'num_urls', 'num_mentions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ad247a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multichannel Neural Network with bert input\n",
    "\n",
    "#Creating the model for bert embedding\n",
    "\n",
    "input_tweet = Input(shape = (120, 128))\n",
    "mask = Masking(mask_value=-10)(input_tweet)\n",
    "lstm = LSTM(32)(mask)\n",
    "input_metadata = Input(shape = (6,))\n",
    "combined = concatenate([input_metadata, lstm])\n",
    "dense1 = Dense(units = 128, activation = 'relu')(combined)\n",
    "dense2 = Dense(units = 64, activation = 'relu') (dense1)\n",
    "#The final cell is the output cell with a softmax \n",
    "output = Dense(units = 1, activation = 'sigmoid')(dense2)\n",
    "\n",
    "model = Model(inputs = [input_tweet, input_metadata], outputs = output)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e65c4caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1250/1250 [==============================] - 82s 60ms/step - loss: 0.5303 - acc: 0.7289\n",
      "Epoch 2/15\n",
      "1250/1250 [==============================] - 73s 59ms/step - loss: 0.4725 - acc: 0.7695\n",
      "Epoch 3/15\n",
      "1250/1250 [==============================] - 88s 70ms/step - loss: 0.4466 - acc: 0.7849\n",
      "Epoch 4/15\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.4264 - acc: 0.7980\n",
      "Epoch 5/15\n",
      "1250/1250 [==============================] - 82s 66ms/step - loss: 0.4087 - acc: 0.8075\n",
      "Epoch 6/15\n",
      "1250/1250 [==============================] - 84s 67ms/step - loss: 0.3930 - acc: 0.8182\n",
      "Epoch 7/15\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.3771 - acc: 0.8265\n",
      "Epoch 8/15\n",
      "1250/1250 [==============================] - 79s 63ms/step - loss: 0.3639 - acc: 0.8350\n",
      "Epoch 9/15\n",
      "1250/1250 [==============================] - 77s 62ms/step - loss: 0.3505 - acc: 0.8424\n",
      "Epoch 10/15\n",
      "1250/1250 [==============================] - 79s 64ms/step - loss: 0.3378 - acc: 0.8471\n",
      "Epoch 11/15\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 0.3262 - acc: 0.8577\n",
      "Epoch 12/15\n",
      "1250/1250 [==============================] - 85s 68ms/step - loss: 0.3149 - acc: 0.8617\n",
      "Epoch 13/15\n",
      "1250/1250 [==============================] - 90s 72ms/step - loss: 0.3019 - acc: 0.8674\n",
      "Epoch 14/15\n",
      "1250/1250 [==============================] - 84s 67ms/step - loss: 0.2926 - acc: 0.8724\n",
      "Epoch 15/15\n",
      "1250/1250 [==============================] - 77s 62ms/step - loss: 0.2813 - acc: 0.8781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19f8cbdcdf0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = [feature_vectors_train, meta_data_train], y = y_train, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "412eb74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 11s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict([feature_vectors_test, meta_data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2eafca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.where(y_pred_prob > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eb522627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81      5094\n",
      "           1       0.80      0.81      0.81      4906\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.81      0.81      0.81     10000\n",
      "weighted avg       0.81      0.81      0.81     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "33aa418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8072\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b9f4a",
   "metadata": {},
   "source": [
    "# Training a model purely on metadata for comparison\n",
    "\n",
    "Here, we shall train a xgboost model purely on the tweet data in order to compare with the results of the multichannel network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c052d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_metadata = XGBClassifier(gamma = 0.1,\n",
    "                               alpha = 0.5,\n",
    "                               max_depth = 25, \n",
    "                               eta = 0.01, \n",
    "                               subsample = 0.8,\n",
    "                               colsample_bytree = 1.0,\n",
    "                               \n",
    "                               objective = \"binary:logistic\",\n",
    "                               eval_metric = \"logloss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c917dd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.5, base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1.0,\n",
       "              early_stopping_rounds=None, enable_categorical=False, eta=0.01,\n",
       "              eval_metric='logloss', feature_types=None, gamma=0.1, gpu_id=-1,\n",
       "              grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.00999999978,\n",
       "              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=25, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, ...)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_metadata.fit(meta_data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b2acfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_metadata.predict(meta_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8d096ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.70      0.77      5000\n",
      "           1       0.75      0.88      0.81      5000\n",
      "\n",
      "    accuracy                           0.79     10000\n",
      "   macro avg       0.80      0.79      0.79     10000\n",
      "weighted avg       0.80      0.79      0.79     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd992a8",
   "metadata": {},
   "source": [
    "While the results are better than the pure text LSTM models, the multichannel network outperforms this model, in particular for the human class, where the f1-score is significantly higher."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
